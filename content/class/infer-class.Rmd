---
title: "Bootstrapping and hypothesis testing"
date: "2019-11-21"
citeproc: false
bibliography: ../../static/bib/references.bib
csl: ../../static/bib/chicago-syllabus-no-bib.csl
output:
  blogdown::html_page:
    template: ../../pandoc/toc-title_html.template
    toc: true
editor_options: 
  chunk_output_type: console
---

# Slides


[Download slides from today's lecture](/images/slides/21-sampling.pdf)



# Code from class


## Bootstrapping with the infer package



Here's how to do bootstrapping with the `infer` package. Note that each time you will get slightly different answers. 

```{r, eval=FALSE}
# bootstrapping with infer
library(infer)

boot_childs = gss_sm %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean")

boot_childs

# visualize results: with ggplot
ggplot(boot_childs, aes(x = stat)) +
  geom_histogram(color = "white")


# with visualize(
visualize(boot_childs)


# calculate the standard error
boot_childs %>% 
  summarise(se = sd(stat))
```


As you might expect, sampling variability will increase with a smaller sample (compare standard error to above):


```{r, eval=FALSE}
# variability increases when we have a smaller sample
boot_childs_sm = gss_sm %>% 
  # Get a smaller sample
  sample_n(250) %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean")


# plot
ggplot(boot_childs_sm, aes(x = stat)) +
  geom_histogram(color = "white") +
  labs(title = "N = 250 respondents. SE = .12")


# standard error
boot_childs_sm %>% 
  summarise(se = sd(stat))

```



We can also get confidence intervals for our estimate.


```{r, eval=FALSE}
# calculate confidence interval
percentile_ci = boot_childs %>%
  get_ci(level = 0.95, type = "percentile")

# visualize it
visualize(boot_childs) + 
  shade_confidence_interval(endpoints = percentile_ci)

# confidence intervals of different sizes
boot_childs %>%
  get_ci(level = 0.95, type = "percentile")
boot_childs %>%
  get_ci(level = 0.99, type = "percentile")
boot_childs %>%
  get_ci(level = 0.50, type = "percentile")


```




We can do bootstrapping with different statistics, not just means. For example:


```{r, eval=FALSE}
# other infer: diff in means
gss_sm %>% 
  specify(response = obama, explanatory = sex) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in means", order = c("Female", "Male")) %>% 
  visualize()

# slopes
gss_sm %>% 
  specify(childs ~ age) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "slope") %>%
  summarise(se = sd(stat))
```



Confidence intervals will also change as the sample size increases or decreases.

```{r, eval=FALSE}
# different sized samples
gss_sm %>% 
  sample_n(1000) %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean") %>% 
  get_ci(level = 0.95, type = "percentile")


gss_sm %>% 
  sample_n(500) %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean") %>% 
  get_ci(level = 0.95, type = "percentile")


gss_sm %>% 
  sample_n(200) %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean") %>% 
  get_ci(level = 0.95, type = "percentile")

gss_sm %>% 
  sample_n(50) %>% 
  # Specify the variable of interest
  specify(response = childs) %>% 
  # Generate a bunch of bootstrap samples
  generate(reps = 1000, type = "bootstrap") %>% 
  # Find the average of each sample
  calculate(stat = "mean") %>% 
  get_ci(level = 0.95, type = "percentile")
```


## Hypothesis testing with the infer package



From class, we wanted to know, do people with children have affairs at lower rates than people without?


```{r, eval=FALSE}
library(tidyverse)
library(socviz)
library(moderndive)
library(infer)
library(AER)


# hypothesis testing
data("Affairs")
Affairs %>% 
  group_by(children) %>% 
  summarise(affairs = mean(affairs)) %>% 
  ggplot(aes(x = children, y = affairs)) + 
  geom_col() + 
  theme_bw(base_size = 15)
```


How sure are we of this estimate? What if we got a weird sample? Imagine if we randomly reshuffled whether or not each person in the sample had kids or not.


```{r, eval=FALSE}
# shuffling number of kids
shuffled_affairs = Affairs %>% 
  select(affairs, children) %>% 
  mutate(children = sample(c("yes", "no"), size = n(), replace = TRUE))

shuffled_affairs %>% 
  group_by(children) %>% 
  summarise(affairs = mean(affairs)) %>% 
  ggplot(aes(x = children, y = affairs)) + 
  geom_col() + 
  theme_bw(base_size = 15)
```


This is the basis of permutation-based hypothesis testing. We make lots of copies of our data, randomly shuffle our explanatory variable, and see what kinds of results we might get. This is the `null hypothesis`: what we might observe simply by chance. 

In the `infer` package, the basic setup is below:


```{r, eval=FALSE}
# permutation based hypothesis testing
null_affairs = Affairs %>% 
  specify(formula = affairs ~ children) %>% 
  # note the new call here to hypothesize no relationship between the two vars ("independence")
  hypothesize(null = "independence") %>% 
  # note here we are using "permute" instead of "bootstrap"
  generate(reps = 1000, type = "permute") %>% 
  # note that we are computing a difference in means (avg. affairs with kids - avg. affairs with no kids)
  calculate(stat = "diff in means", order = c("yes", "no"))

# plotting the results
ggplot(null_affairs, aes(x = stat)) + 
  geom_histogram(color = "white", binwidth = .1) + 
  geom_vline(xintercept = .76, color = "red", size = 2, lty = 2) + 
  theme_bw(base_size = 15)

# OR
visualize(null_affairs) + geom_vline(xintercept = .76, color = "red", size = 2, lty = 2)
```



We can get our p-value using infer as well.

```{r, eval=FALSE}
# visualize p-value
visualize(null_affairs) + 
  shade_p_value(obs_stat = .76, direction = "right") + 
  theme_bw(base_size = 15)

# get p-value
null_affairs %>% 
  get_p_value(obs_stat = .76, direction = "both")
```


Remember, the p-value is the probability of observing a value as extreme as the one we found *by chance*. 